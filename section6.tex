%\documentclass[twocolumn,~ 11pt,~a4paper]{jreport}
\documentclass[11pt,a4paper]{jreport}
\usepackage{amsmath,amssymb}
\usepackage{newtxtext,newtxmath}
\usepackage[dvipdfmx]{graphicx}
\usepackage{listings}
\usepackage[top=30truemm,bottom=30truemm,left=20truemm,right=20truemm]{geometry}
\begin{document}
% \setlength {\columnsep}{2zw}
%\twocolumn[
\title{PRML輪読会~資料}
\author{西本　洋紀 }
\maketitle
\chapter*{第6章~~~カーネル法}
% ]
\subsection*{導入}
第3章,~ 第4章で議論した線形回帰,~分類問題では,~入力$\bf{x}から出力yへの写像y(\bf{x},~ \bf{w})を$以下のように定式化した.~\\
\begin{equation}
  y(\bf{x},~ \bf{y}) = f(\bf{w}^T \phi(\bf{x}))
\end{equation}
但し,~ $\phi(\bf{x})$は基底関数である.~ $f(\cdot)$は,~ 回帰問題では恒等関数,~分類問題では非線形活性化関数である.~ 学習の際は,~ 訓練データはパラメータ分布の点推定値,~ あるいは事後分布を得るために利用される.~ 訓練後には,~訓練データは捨てられ,~新しい入力に対する予測は学習済みのパラメータベクトル$\bf{w}$のみを用いて行われる.~ このアプローチは,~ 第5章で扱ったニューラルネットワークのような非線形のパラメトリックモデルにおいても同様である.~\par
一方で,~ 訓練データ点の一部ないし全部を,~ 予測時にも利用するようなパターン認識法のクラスが存在する.~ これは\bf{メモリベース法}と呼ばれ,~ 全ての訓練データを予測時まで保存しておく必要がある.~ 一般に,~ 「訓練」は早く行うことができる反面,~テスト点に対する予測には時間がかかる.~ \par
$通常,~ これらのメモリベース法では,~ 入力空間における任意の2つのベクトルの類似度を測る指標が必要になる.~ この指標に\bf{カーネル関数}を用いる方法を\bf{カーネル法}といい,~ 本稿で詳しく議論する.~ カーネル関数は,~入力ベクトル\bf{x}からあらかじめ定義された非線形の特徴空間への写像\phi(\bf{x})に基づくモデルにおいて,~ 下式(2)のように定義される.~$
\begin{equation}
  k(\bf{x},~ \bf{x}') = \phi(\bf{x}')^T \phi(\bf{x}')
\end{equation}
%\newpage
多くのカーネル関数は,~ 引数の差に依存したもの,~ つまり,~ $k(\bf{x},~ \bf{x}') = k(\bf{x} - \bf{x}') $の形をもったもので,~ いくつかの入力空間の返還に関して不変であることから\bf{不変カーネル}と呼ばれる.~ 一方,~ $k(\bf{x},~ \bf{x}') = k(\|\bf{x} - \bf{x}'\|)$ のように,~ 2つの入力ベクトルの間の距離(主にユークリッド距離)にのみ依存するカーネルは,~ \bf{均一カーネル}あるいは\bf{RBF}(動径基底関数; radial basis function)と呼ばれる.~
\subsection*{本稿の構成}
6.1節では、回帰で用いられる線形モデルを双対表現で表すことで,~ カーネル関数が自然に現れてくることを確認する.~ 6.2節では,~ 有効なカーネル関数の様々な構成法について述べる.~ 続いて6.3節では,~ 線形回帰モデルの基底関数としてどのようなものを取れば良いかという問題を扱う.~ 一般的に,~ 基底関数としてよく利用されるのはRBFである.~ RBFを導入する動機について説明し,~ その過程で Nadaraya-Watsonモデルを導く.~ このモデルは,~ カーネル回帰モデルをカーネル密度推定の観点から動機づけることによっても導出され,~ カーネル回帰とも呼ばれる.~ 6.4節では,~ 6.1節の発展として,~ カーネルを確率的識別モデルに対しても適用することで,~ガウス過程を導き,~ ベイズ的な設定においても,~ 自然にカーネルが現れてくることを見る.~
%\newpage
\section*{6.1~~~双対表現}
線形回帰モデルや線形分類モデルを双対表現で表すことで,~ カーネル関数が自然に現れてくる.~ このことは,~ 第7章で扱うサポートベクトルマシンにおいて重要な意味を持つ.~ 本節では特に,~ 回帰のための非確率的線形モデルに対して双対性の概念を用いることでカーネル法を導く.~ \par
まず、下式(3)のように,~ 誤差関数が二乗和誤差と2次の正則化項の和で表せるような線形回帰のモデルを考える.~
\begin{equation}
  J(\bf{w}) = \frac{1}{2} \sum_{n=1}^{N} \{\bf{w}^T \phi (\bf{x}_n)-t_n\}^2 + \frac{\lambda}{2}\bf{w}^T\bf{w}
\end{equation}
なお,~ $\lambda\geq0$であるとする.~\par
式(3)の誤差関数$J(\bf{w})の\bf{w}についての勾配を零とおくと,~ \bf{w}は,~ 以下のように,~ 係数が\bf{w}の関数であるような,~ ベクトル集合\phi(\bf{x}_n)$の線型結合の和になることがわかる.~
\begin{equation}
  \begin{split}
    \bf{w}&=-\frac{1}{\lambda}\sum_{n=1}^{N} \{\bf{w}^T \phi (\bf{x}_n)-t_n\} \phi (\bf{x}_n)\\
    &= \sum_{n=1}^{N} a_n \phi (\bf{x}_n)\\
    &= \Phi^T\bf{a}
  \end{split}
\end{equation}
ここで,~ $ \Phi は,~ n番目の行が\phi(\bf{x}_n)^Tで$与えられるような計画行列である.~ また,~
\begin{equation}
  a_n = \frac{1}{\lambda} \{ \bf{w}^T\phi(\bf{x}_n) - t_n\}
\end{equation}
として,~ そのベクトル形式を$\bf{a}=\{ a_1,.~.~.~,a_N\}^Tとする.~$\par
$\bf{w} = \Phi^T\bf{a}を用いて~(\bf{w})をJ(\bf{a})$に変換すると,~下式のようになる.
\begin{equation}
  J(\bf{a}) = \frac{1}{2}\bf{a}^T\Phi\Phi^T\Phi\Phi^T\bf{a}-\bf{a}^T\Phi\Phi^T\bf{t}+\frac{1}{2}\bf{t}^T\bf{t}+\frac{\lambda}{2}\bf{a}^T\Phi\Phi^T\bf{a}
\end{equation}
ただし,~$\bf{t} = \{ t_1,...,t_N\}であるとする.~ここで、 N \times N の対称行列であり, その要素が$
\begin{equation}
  K_{nm} = \phi(\bf{x}_n)^T \phi(\bf{x}_m)= k(\bf{x}_n, \bf{x}_m)
\end{equation}
$で定められる\bf{グラム行列}\bf{K}=\Phi\Phi^T$を定義する.~なおここで, 式(2)で定義される\bf{カーネル関数}$k(\bf{x}, \bf{x}')$を使った.~グラム行列を用いると,~二乗和誤差関数は
\begin{equation}
  J(\bf{a})=\frac{1}{2}\bf{a}^T\bf{K}\bf{K}\bf{a} - \bf{a}^T\bf{K}\bf{t} + \frac{1}{2}\bf{t}^T\bf{t}+\frac{\lambda}{2}\bf{a}\bf{K}\bf{a}
\end{equation}
と少し簡単に書くことができる.\par
$さて,~ パラメータベクトル\bf{w}を直接扱う代わりに,~ 最小二乗法のアルゴリズムをパラメータベクトル\bf{a}で表現し直すことにする.~ これは\bf{双対表現}と呼ばれる.~ J(\bf{w})をJ(\bf{a})に変換して,~ \frac{\partial J(\bf{a})}{\partial \bf{a}} =0を\bf{a}について解いても良いが,~~式(3)を利用すればより簡単に双対表現が求められる.~ すなわち,~ \bf{w} = \Phi^T\bf{a}を
%\frac{\partial J(\bf(w))}{\partial \bf(w)}=0として得られた
式(3)に代入して,~ \bf{a}について解けば良い.~ 結果は$
\begin{equation}
\bf{a}=(\bf{K} + \lambda\bf{I}_N)^{-1}\bf{t}
\end{equation}
となる.~
式(9)の双対表現を線形回帰モデルに代入し直すことによって, 新しい入力\bf{x}に対する予測は次のように与えられることがわかる.
\begin{equation}
  y(\bf{x})=\bf{w}^T\phi(\bf{x})=\bf{a}\Phi\phi(\bf{x})=\bf{k}(\bf{x})^T(\bf{K}+\lambda\bf{I}_N)^{-1}\bf{t}
\end{equation}
ここで, $\bf{k}(\bf{x})は要素k_n=k(\bf{x}_n,\bf{x})を持つベクトルである.$\\
以上より,~双対表現に変換することで, 最小二乗法の解はカーネル関数$k(\bf{x},\bf{x}')のみによって表現できることがわかる.~これが双対表現と呼ばれる理由は,\phi(\bf{x})の要素の線型結合によって\bf{a}がひょうげmんmできることから,~パラメータベクトル\bf{w}を使った, 元々の定式化を復元できるためである.$\par
$双対表現に変換すると, パラメータベクトル\bf{a}を求めるのにN \times Nの逆行列を求める必要がある.~一方,~元々のパラメータ空間における定式化では,~M \times M行列の逆行列を求めることによって\bf{w}が得られる.~通常NはMよりもずっと大きいので, 双対表現にすると計算量が大きくなることになり, 一見不都合であるかのように見える.~しかし,~双対表現の意義は, 全てがカーネル関数k(\bf{x},\bf{x}')で表現されることにある.~これにより, 特徴ベクトル\phi(\bf{x})を明示的に考えることを避け, 高次元の, 時には無次元の特徴空間を間接的に扱うことができる.$\par
\section*{6.~2~~~カーネル関数の構成}
実際にカーネル置換を行うにはカーネル関数として有効なものを構成する必要があり,~本節では,~そのための様々なアプローチをみる.\\
\begin{itemize}
  \item 特徴空間への写像$\phi(\bf{x})$をもとに,~対応するカーネルを設計する方法. \\
  例えば, 一次元の入力空間において,~次のように\\
  \begin{equation}
    k(x,x')=\bf{\phi}(x)^T \bf{\phi}(x')=\sum_{i=1}^{M} \phi _i(x) \phi _i(x')
  \end{equation}
  \item カーネル関数を直接定義する方法\\

  \item 単純なカーネル関数を構成要素として, 新たなカーネル関数を構築する方法\\

  \item 確率的生成モデルからカーネル関数を構成する方法\\
  次の2つの方法がある.
  \begin{enumerate}
    \item 生成モデル$p(\bf{x})が与えられた時に,~k(\bf{x},\bf{x}')=p(\bf{x})p(\bf{x}')$によりカーネル関数を定義する方法
    \item フィッシャーカーネル

  \end{enumerate}
\end{itemize}
\section*{6.~3~~~RBFネットワーク}

\section*{6.~4~~~ガウス過程}
本節では,回帰のための確率的な線形モデルと,~ガウス過程との双対性を示す.
\end{document}
